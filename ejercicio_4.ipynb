{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b142b964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo necesario en caso de problemas con los certificados SSL\n",
    "import os\n",
    "import certifi\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "os.environ['HF_HOME'] = 'D:\\\\huggingface_cache' # Cambia esta ruta a la que prefieras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e07a309",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\git\\fundamentos-llms\\fund-llms\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo utilizado: cuda\n",
      "Nombre del dispositivo: NVIDIA T1200 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Agregamos una prueba para verificar si estamos usando cuda o cpu\n",
    "# e imprimimos el dispositivo que se está utilizando así como su nombre\n",
    "\n",
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Dispositivo utilizado:\", \"cuda\" if device == 0 else \"cpu\")\n",
    "if device == 0:\n",
    "    print(\"Nombre del dispositivo:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96125ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['I', 'Ġlove', 'Ġcoding', 'Ġin', 'Ġpython']\n",
      "Token IDs: [40, 1842, 19617, 287, 21015]\n",
      "Reconstructed Tokens: ['I', 'Ġlove', 'Ġcoding', 'Ġin', 'Ġpython']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "example_sentence = \"I love coding in python\"\n",
    "tokens = tokenizer.tokenize(example_sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "reconstructed_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfbfc65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['O', 'th', 'on', 'Ġloves', 'Ġcoding', 'Ġin', 'Ġpython']\n",
      "Token IDs: [46, 400, 261, 10408, 19617, 287, 21015]\n",
      "Reconstructed Tokens: ['O', 'th', 'on', 'Ġloves', 'Ġcoding', 'Ġin', 'Ġpython']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "oov_sentence = \"Othon loves coding in python\"\n",
    "tokens = tokenizer.tokenize(oov_sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "reconstructed_tokens = tokenizer.convert_ids_to_tokens(token_ids)\n",
    "\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Token IDs:\", token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ee1773c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplo de autoregresión con GPT-2\n",
    "from transformers import set_seed, GPT2LMHeadModel, pipeline\n",
    "from torch import tensor, numel\n",
    "from bertviz import model_view\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "31587eae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "d:\\git\\fundamentos-llms\\fund-llms\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:83: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Output 1: The current amount of data available for training language models is too small to be considered a sufficient sample size, and is therefore not subject to a large-scale study. The current results are in addition to the\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Output 2: The current amount of data available for training language models is limited, so the model is not completely accurate. We are looking for a new approach, such as training with a single variable, and using a dataset\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n",
      "\n",
      "Output 3: The current amount of data available for training language models is limited by the number of training methods used. Therefore, we are currently working to increase the number of training methods for training language models by implementing training methods\n",
      "\n",
      "\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='gpt2', device=device)\n",
    "\n",
    "# Imprimimos las salidas del generador de texto\n",
    "outputs = generator(\n",
    "    \"The current amount of data available for training language models is\",\n",
    "    max_new_tokens=30,\n",
    "    num_return_sequences=3,\n",
    "    truncation=True\n",
    ")\n",
    "\n",
    "for i, output in enumerate(outputs):\n",
    "    print(f\"\\nOutput {i + 1}: {output['generated_text']}\\n\\n\")\n",
    "    print(\"----------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14dc08c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Descargaremos el modelo GPT-2 y lo cargaremos manualmente para visualizarlo\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", output_attentions=True)\n",
    "model.to(\"cuda\" if device == 0 else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0ff112",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 768])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Es importante siempre manejar los tensores en el mismo dispositivo que el modelo\n",
    "model_device = \"cuda\" if device == 0 else \"cpu\"\n",
    "\n",
    "# Movemos el tensor al mismo dispositivo\n",
    "encoded_input = tokenizer(\"Othon loves to code.\", return_tensors='pt')\n",
    "input_ids = encoded_input[\"input_ids\"].to(model_device)\n",
    "\n",
    "# Obtenemos los embeddings de posición (wpe) para cada una de las palabras\n",
    "position_ids = torch.arange(input_ids.size(1), device=input_ids.device).unsqueeze(0).expand_as(input_ids)\n",
    "model.transformer.wpe(position_ids).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65a0f9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "# Obtenemos ahora los embeddings de tokens (wte) para cada palabra\n",
    "wte_encoded = model.transformer.wte(input_ids)\n",
    "print(wte_encoded.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c661f2a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "# El input inicial es la suma del embedding de tokens y el embedding de posición\n",
    "initial_input = model.transformer.wte(input_ids) + model.transformer.wpe(position_ids)\n",
    "print(initial_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d3a62599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0461, -0.2066,  0.1599,  ..., -0.1851,  0.1534,  0.0842],\n",
       "         [-0.0048, -0.1885,  0.1429,  ...,  0.0197, -0.1129, -0.0914],\n",
       "         [-0.2206, -0.0326,  0.2140,  ..., -0.2712, -0.1248, -0.1405],\n",
       "         ...,\n",
       "         [-0.0008, -0.1269,  0.1579,  ..., -0.0074,  0.1089,  0.0720],\n",
       "         [-0.0841,  0.0145,  0.1413,  ..., -0.3213,  0.1117,  0.1050],\n",
       "         [ 0.0493, -0.0318,  0.1479,  ..., -0.0710,  0.0533,  0.0938]]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "initial_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98ba0389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de parámetros en el modelo: 124,439,808\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(numel(p) for p in model.parameters())\n",
    "print(f\"Número total de parámetros en el modelo: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c9e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fund-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
