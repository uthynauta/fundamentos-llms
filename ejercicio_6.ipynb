{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad77185a",
   "metadata": {},
   "source": [
    "# Fine-tuning en GPT\n",
    "\n",
    "El proceso de Fine-tuning nos permite tomar un modelo preexistente como GPT-2, y hacer ajustes en sus pesos para que sea capaz de aprender nuevas relaciones entre tokens e incluso aumentar el contexto de algunas palabras que antes carec√≠an de √©l. Por ejemplo GPT-2 fue entrenado en un momento en el que no se hablaba mucho respecto a los chatbots o herramientas de NLP, por lo tanto al enfrentarse a algunos conceptos como _tranformers_, _fine-tuning_, _tokens_, etc., podr√≠a no interpretarlos de forma correcta.\n",
    "\n",
    "Podemos, mediante el proceso de fine-tuning ajustar los pesos del modelo (o parte del mismo) para que \"aprenda\" estos nuevos conceptos y mejore la generaci√≥n de un texto en este contexto espec√≠fico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2c9e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\git\\fundamentos-llms\\fund-llms\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, \\\n",
    "    Trainer, TrainingArguments, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b48f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo necesario en caso de problemas con los certificados SSL\n",
    "import os\n",
    "import certifi\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "os.environ['HF_HOME'] = 'D:\\\\huggingface_cache' # Cambia esta ruta a la que prefieras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17cfb119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo utilizado: cuda\n",
      "Nombre del dispositivo: NVIDIA T1200 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Agregamos una prueba para verificar si estamos usando cuda o cpu\n",
    "# e imprimimos el dispositivo que se est√° utilizando as√≠ como su nombre\n",
    "\n",
    "import torch\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "print(\"Dispositivo utilizado:\", \"cuda\" if device == 0 else \"cpu\")\n",
    "if device == 0:\n",
    "    print(\"Nombre del dispositivo:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb74c2e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\git\\fundamentos-llms\\fund-llms\\Lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ü§ó Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# GPT-2 no tiene pad_token por defecto, as√≠ que lo asignamos al eos_token\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "nlp_data = TextDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    file_path=\"data\\\\The_Evolution_of_Natural_Language_Processing.txt\",  # Cambia esta ruta a la que prefieras\n",
    "    block_size=128,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b8b4371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primer elemento del dataset (token IDs): tensor([28900, 34786,   420, 36876,    16,   198,    16,   198,   198, 35191,\n",
      "        15547,   407,  1695,   198,   198, 16192,  1160,    11,  1160,  1954,\n",
      "          198, 23839,   198,   198, 14231,   319,  1160,  1526,  1160,  1954,\n",
      "          851, 12624,    12, 17513,   604,    13,    15,   851,  3740,  1378,\n",
      "        34023,    13,  2398,    14,   940,    13, 18182,  3901,    14,   559,\n",
      "           13,  1433,  3720,  2327, 34229,    13, 34716, 38569,  4051,    14,\n",
      "           85,    16,   851,   770,   257,   662,  4798,   290,   468,   407,\n",
      "          587, 12720, 11765,    13,  6060,   743,   307, 15223,    13,   198,\n",
      "          198,  1212,  3188, 13692,   319, 24101,    38, 11571,    11,   257,\n",
      "         3288,  3303,  7587,   357,    45, 19930,     8,  2746,  3170,   416,\n",
      "          262, 47385, 17019,  3127,    13,   198,   464,  3188,  3769,   257,\n",
      "         9815, 16700,   286,   262, 10959,    11,  3047,    11,   290,  3734,\n",
      "           12, 28286,   278,   286, 24101,    38, 11571,    11])\n",
      "\n",
      "Primer elemento del dataset (texto): Ho Ngoc Hai1\n",
      "1\n",
      "\n",
      "Affiliation not available\n",
      "\n",
      "March 20, 2023\n",
      "Abstract\n",
      "\n",
      "Posted on 20 Mar 2023 ‚Äî CC-BY 4.0 ‚Äî https://doi.org/10.22541/au.167935454.46075854/v1 ‚Äî This a preprint and has not been peer reviewed. Data may be preliminary.\n",
      "\n",
      "This document focuses on ChatGPT, a natural language processing (NLP) model built by the transformer neural network.\n",
      "The document provides a comprehensive overview of the architecture, training, and fine-tuning of ChatGPT,\n",
      "\n",
      "N√∫mero de tokens en el primer elemento del dataset: 128\n",
      "\n",
      "N√∫mero de ejemplos en el dataset: 300\n"
     ]
    }
   ],
   "source": [
    "# Inspeccionemos el primero elemento del dataset\n",
    "print(\"Primer elemento del dataset (token IDs):\", nlp_data[0])\n",
    "print(\"\\nPrimer elemento del dataset (texto):\", tokenizer.decode(nlp_data[0]))\n",
    "print(\"\\nN√∫mero de tokens en el primer elemento del dataset:\", len(nlp_data[0]))\n",
    "\n",
    "# Veamos cu√°ntos ejemplos hay en el dataset\n",
    "print(\"\\nN√∫mero de ejemplos en el dataset:\", len(nlp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27eb726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['output_attentions']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    }
   ],
   "source": [
    "from transformers import set_seed, GPT2LMHeadModel, pipeline\n",
    "from torch import tensor, numel\n",
    "from bertviz import model_view\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "from transformers import GPT2Config\n",
    "config = GPT2Config.from_pretrained(\"gpt2\", attn_implementation=\"eager\", output_attentions=True)\n",
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\", config=config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08e644c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=False,\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "285c189e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 1212,   318,   257,  4738,  5128],\n",
       "        [ 1212,   318,  1194,   530, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0]]), 'labels': tensor([[1212,  318,  257, 4738, 5128],\n",
       "        [1212,  318, 1194,  530, -100]])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "collator_example = data_collator([tokenizer('This is a random input'), tokenizer('This is another one')])\n",
    "\n",
    "collator_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfdd237e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Nuestro token de padding es el mismo que el token de fin de secuencia (eos_token)\n",
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b667ff22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Asimismo, nuestro tensor de atenci√≥n debe ignorar los tokens de padding\n",
    "collator_example['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cac68d3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
    "pretrained_generator = pipeline(\n",
    "    'text-generation', model=model, tokenizer=tokenizer, device=device, \n",
    "    config={'max_length': 20, 'do_sample': True, 'top_k': 10, 'top_p': 0.9, 'temperature': 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f0f746",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\git\\fundamentos-llms\\fund-llms\\Lib\\site-packages\\transformers\\integrations\\sdpa_attention.py:83: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets biases are a problem in both the physical sciences and computer science. In the physical sciences, we have an open field that enables us to analyze and interpret complex computational models. In computer science, we have a number of closed field models where we can only do a series of observations and not even model the entire network. However, there are many applications to these data and it has been recently observed that the number of models of this kind can be greatly increased due to the fact that these data are freely available online. In this post, I will discuss this open field model and how it can be applied to real data sets.\n",
      "\n",
      "The Open Field Model\n",
      "\n",
      "The Open Field Model is a model of data, which is a way of categorizing data and identifying the specific variables in a continuous set. We use the term 'data' to describe a set of variables that can be grouped together in a continuous set. In the Open Field Model, we can classify data by the number of observations, the number of variables in the data, the number of variables in the data, the number of variables in the data and the number of variables in the data, and the number of variables in the data. When we use the term 'data' to describe a set of variables, we can identify the variables by\n",
      "--------------------\n",
      "Datasets biases are a problem in the real world. You can easily make a mistake when doing so. The best way to do it is to make your data look like it's from the beginning.\n",
      "\n",
      "Let's say you have a database, a table, and a series of fields. For an algorithm to know whether the table is from a data set or from one, you'd need to know how many rows and columns you have.\n",
      "\n",
      "To do this, you'll need to know how many rows and columns a particular table has.\n",
      "\n",
      "For this example, we'll use a table that has three fields:\n",
      "\n",
      "data = { names = \"Mountain Dew\", age = 30, name_id = \"P.A\", type = \"data\", name_id = \"Mountain Dew\", field_type = \"data\" }; data.names = data;\n",
      "\n",
      "Here, we're looking at two rows and a column and a name:\n",
      "\n",
      "If you look closely, you'll see that the fields are all integers, and the field_type is not a string. The fields are unique, but the name is unique too.\n",
      "\n",
      "Using a column as a data set\n",
      "\n",
      "You can find this algorithm in a lot of programming languages, but the simplest way to\n",
      "--------------------\n",
      "Datasets biases are a problem in a number of ways. First, as we explained in our introduction, data is not necessarily representative of the human brain. On the contrary, we know that the human brain is biased from its earliest stages, but we can't fully see how this happens in our experiments.\n",
      "\n",
      "What we can do is to explore how humans can improve their ability to learn and process information from the outside world. We can learn from the world around us. We can learn from our peers. We can learn from the world around us. And if we can learn from the world around us, then we can learn from our peers and from the world around them.\n",
      "\n",
      "This brings us to a second question: how can we improve our ability to learn and process information from the outside world? We can learn from our peers, and from everyone. We can learn from our colleagues, and from the world around us. We can learn from our children. We can learn from our teachers, and from the world around us.\n",
      "\n",
      "As we discuss the above, we can learn from our peers and from the world around us. And as we discuss the above, we can learn from all of us. And as we discuss the above, we can learn from all of the people around us.\n",
      "\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------')\n",
    "for generated_sequence in pretrained_generator(\"Datasets biases are a problem in\", num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc25f15e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 3.4060862064361572,\n",
       " 'eval_model_preparation_time': 0.003,\n",
       " 'eval_runtime': 1.5585,\n",
       " 'eval_samples_per_second': 38.499,\n",
       " 'eval_steps_per_second': 2.567}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./models/gpt2-finetuned\",  # Cambia esta ruta a la que prefieras\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    warmup_steps=len(nlp_data)//16,  # 1 epoch de warmup\n",
    "    logging_steps=5\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=nlp_data.examples[:int(0.8*len(nlp_data))],\n",
    "    eval_dataset=nlp_data.examples[int(0.8*len(nlp_data)):]\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "85c2c316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='45' max='45' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [45/45 07:26, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.786800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.544100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.266000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.099200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>2.823600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>2.769400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.587100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.580200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=45, training_loss=3.034053760104709, metrics={'train_runtime': 451.0996, 'train_samples_per_second': 1.596, 'train_steps_per_second': 0.1, 'total_flos': 47032565760000.0, 'train_loss': 3.034053760104709, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fb7525ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0.3 - Training Loss: 3.7868\n",
      "Epoch: 0.7 - Training Loss: 3.5441\n",
      "Epoch: 1.0 - Training Loss: 3.2660\n",
      "Epoch: 1.3 - Training Loss: 3.0992\n",
      "Epoch: 1.7 - Training Loss: 2.8236\n",
      "Epoch: 2.0 - Training Loss: 2.8500\n",
      "Epoch: 2.3 - Training Loss: 2.7694\n",
      "Epoch: 2.7 - Training Loss: 2.5871\n",
      "Epoch: 3.0 - Training Loss: 2.5802\n"
     ]
    }
   ],
   "source": [
    "# Veamos como ha ido el entrenamiento\n",
    "for log in trainer.state.log_history:\n",
    "    if 'loss' in log and 'epoch' in log:\n",
    "        print(f\"Epoch: {log['epoch']:.1f} - Training Loss: {log['loss']:.4f}\")\n",
    "    if 'eval_loss' in log and 'epoch' in log:\n",
    "        print(f\"Epoch: {log['epoch']:.1f} - Validation Loss: {log['eval_loss']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "815c7f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "loaded_model = GPT2LMHeadModel.from_pretrained(\"./models/gpt2-finetuned/checkpoint-45\")  # Cambia esta ruta a la que prefieras\n",
    "\n",
    "finetuned_generator = pipeline(\n",
    "    'text-generation', model=loaded_model, tokenizer=tokenizer, \n",
    "    config={'max_length': 20, 'do_sample': True, 'top_k': 10, 'top_p': 0.9, 'temperature': 0.7}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "51ab6b73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Datasets biases are a problem in many languages.\n",
      "\n",
      "Many languages have many features to solve many of these problems.\n",
      "However, there are some limitations in language generation and the amount of support available.\n",
      "This article will provide a detailed overview of many of the most common problems in language generation and development.\n",
      "3.1.1 Learning and\n",
      "Learning is an important aspect of language development because it is the key to understanding of the language.\n",
      "Learning is important to understand the natural language and its features, such as its structure, syntax, morphology, and phonemporal structure.\n",
      "When learning new concepts to a new language, there are many different approaches that can be used to help the user understand them.\n",
      "This article will provide an overview of the most common use of learning and information in language development.\n",
      "3.1.2 Learning and\n",
      "Learning is an important aspect of the language development process.\n",
      "Learning is an important aspect of language development because it is the key to understanding the natural language and its features.\n",
      "For example, many language models, such as WordNet, are based on the input data from a Text Model. In this model, the model is trained to learn how to generate sentences and sentence structures.\n",
      "The model also learns how to represent and represent different natural language information\n",
      "--------------------\n",
      "Datasets biases are a problem in the natural language processing industry, where the models are typically based on previous data and then trained to solve queries.\n",
      "The model\n",
      "models\n",
      "The model is a set of three basic classes for natural language processing:\n",
      "\n",
      "1. Natural Language Model\n",
      "2. Natural Language Model\n",
      "3. Natural Machine\n",
      "4. Natural Language Model\n",
      "To classify Natural Language Models, we will use the following language model\n",
      "model:\n",
      "1. Natural Language Model\n",
      "The natural language model is a set of three basic classes for natural language processing.\n",
      "1. Natural Language Model\n",
      "The natural language model is a set of three basic classes for natural language processing:\n",
      "1. Natural Language Model\n",
      "The natural language model is a set of three basic classes for natural language processing:\n",
      "1. Natural Language Model\n",
      "The natural language model is a set of three basic classes for natural language processing:\n",
      "1. Natural Language Model\n",
      "The natural language model is a set of three basic classes for natural language processing:\n",
      "1. Natural Language Model\n",
      "The natural language model is a set of three basic classes for natural language processing:\n",
      "1. Natural Language Model\n",
      "The natural language model is a set of three basic classes for natural language processing:\n",
      "1. Natural Language Model\n",
      "Natural language model is a\n",
      "--------------------\n",
      "Datasets biases are a problem in the field of natural language processing. Here we use a model to calculate the probability of each word between two words in the sentence. We can choose between different datasets and use the model to extract the probability of each word. The model is then trained to learn the word from the source sentence.\n",
      "For sentences with more than 2 words in the sentence, the model uses different datasets with different input data sets. The input dataset, which can be used to train text model training, contains the data from the input sentence input dataset, and each of the dataset inputs are input data sets. We use the input dataset to train the text model on the input data set.\n",
      "We can use the input dataset to train the text model on the input data set. We use the input dataset to train the text model on the input data set. The input dataset contains the data from the input dataset, as input data. The input dataset contains the data from the input dataset, as input data. The input dataset contains the data from the input dataset, as input data. The input dataset contains all the input data from the input dataset, as input data.\n",
      "\n",
      "To train the text model, we use the input dataset as input data. In the input dataset, we train the text model on\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "print('--------------------')\n",
    "for generated_sequence in pretrained_generator(\"Datasets biases are a problem in\", num_return_sequences=3):\n",
    "    print(generated_sequence['generated_text'])\n",
    "    print('--------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68e359c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fund-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
