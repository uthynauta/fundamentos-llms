{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad77185a",
   "metadata": {},
   "source": [
    "# Modelo T5\n",
    "\n",
    "El modelo T5 se encuentra preentrenado para realizar varias tareas, entre ellas:\n",
    "\n",
    "- Traducción\n",
    "- CoLA / Aceptabilidad lingüistica (si la frase tiene o carece de sentido)\n",
    "- Similitud semántica\n",
    "- Resumir párrafos\n",
    "- Preguntas y respuestas con contexto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f2c9e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39b48f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solo necesario en caso de problemas con los certificados SSL\n",
    "import os\n",
    "import certifi\n",
    "os.environ['REQUESTS_CA_BUNDLE'] = certifi.where()\n",
    "os.environ['HF_HOME'] = 'D:\\\\huggingface_cache' # Cambia esta ruta a la que prefieras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17cfb119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo utilizado: cpu\n"
     ]
    }
   ],
   "source": [
    "# Agregamos una prueba para verificar si estamos usando cuda o cpu\n",
    "# e imprimimos el dispositivo que se está utilizando así como su nombre\n",
    "\n",
    "import torch\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Dispositivo utilizado:\", device)\n",
    "if device == \"cuda\":\n",
    "    print(\"Nombre del dispositivo:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb74c2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(\"t5-base\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b8b4371",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texto original: The Hugging Face Hub is a platform that allows users to share and discover machine learning models and datasets. It provides a central repository for pre-trained models, making it easy for developers to access and use state-of-the-art NLP models without having to train them from scratch.\n"
     ]
    }
   ],
   "source": [
    "# Agrueguemos un ejemplo de un texto para resumir\n",
    "text_for_summary = \"\"\"The Hugging Face Hub is a platform that allows users to share and discover machine\n",
    "learning models and datasets. It provides a central repository for pre-trained models, making it easy for\n",
    "developers to access and use state-of-the-art NLP models without having to train them from scratch.\"\"\"\n",
    "\n",
    "preprocessed_text = text_for_summary.strip().replace(\"\\n\", \" \")\n",
    "print(\"Texto original:\", preprocessed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "109cc879",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['early_stopping']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resumen generado:\n",
      "the Hugging Face Hub is a platform that allows users to share and discover machine learning models and datasets . it provides a central repository for pre-trained models .\n"
     ]
    }
   ],
   "source": [
    "# Utilizaremos prompting para indicarle al modelo que queremos hacer un resumen\n",
    "input_text = \"summarize: \" + preprocessed_text\n",
    "\n",
    "input_ids = base_tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "summary_ids = base_model.generate(input_ids,\n",
    "                                  no_repeat_ngram_size=3,\n",
    "                                  min_length=30,\n",
    "                                  max_length=50, \n",
    "                                  early_stopping=True)\n",
    "\n",
    "output = base_tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(f\"Resumen generado:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4b3cc4",
   "metadata": {},
   "source": [
    "## Traducción de Inglés a Alemán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdc38d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traducción generada:\n",
      "Ich lebe in Deutschland\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\"translate English to German: I live in Germany\", \n",
    "                                  return_tensors=\"pt\").to(device)\n",
    "\n",
    "translation_ids = base_model.generate(input_ids,\n",
    "                                        num_beams=4,\n",
    "                                        no_repeat_ngram_size=3,\n",
    "                                        max_length=20,\n",
    "                                        early_stopping=True)\n",
    "\n",
    "output = base_tokenizer.decode(translation_ids[0], skip_special_tokens=True)\n",
    "print(f\"Traducción generada:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f7c526",
   "metadata": {},
   "source": [
    "## CoLA: Corpus of Linguistic Acceptability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d6e20c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es correcta la frase?:\n",
      "acceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\"cola sentence: I love eating pizza.\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "cola_ids = base_model.generate(input_ids,\n",
    "                               num_beams=4,\n",
    "                               no_repeat_ngram_size=3,\n",
    "                               max_length=20,\n",
    "                               early_stopping=True)\n",
    "\n",
    "output = base_tokenizer.decode(cola_ids[0], skip_special_tokens=True)\n",
    "print(f\"Es correcta la frase?:\\n{output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0be772c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Es correcta la frase?:\n",
      "unacceptable\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\"cola sentence: The want jumping pizza.\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "cola_ids = base_model.generate(input_ids,\n",
    "                               num_beams=4,\n",
    "                               no_repeat_ngram_size=3,\n",
    "                               max_length=20,\n",
    "                               early_stopping=True)\n",
    "\n",
    "output = base_tokenizer.decode(cola_ids[0], skip_special_tokens=True)\n",
    "print(f\"Es correcta la frase?:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3808c8",
   "metadata": {},
   "source": [
    "## STSB - Semantic Text Similarity Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b6daf3db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud semántica? (1-5):\n",
      "4.4\n"
     ]
    }
   ],
   "source": [
    "sentence_one = \"I love eating pizza.\"\n",
    "sentence_two = \"Pizza is one of my favorite foods.\"\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", \n",
    "                                  return_tensors=\"pt\").to(device)\n",
    "\n",
    "stsb_ids = base_model.generate(input_ids,\n",
    "                               max_length=3,\n",
    "                               early_stopping=True)\n",
    "\n",
    "output = base_tokenizer.decode(stsb_ids[0], skip_special_tokens=True)\n",
    "print(f\"Similitud semántica? (1-5):\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bc66813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similitud semántica? (1-5):\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "sentence_one = \"I love eating pizza.\"\n",
    "sentence_two = \"My dog likes to eat meat.\"\n",
    "\n",
    "input_ids = base_tokenizer.encode(f\"stsb sentence1: {sentence_one} sentence2: {sentence_two}\", \n",
    "                                  return_tensors=\"pt\").to(device)\n",
    "\n",
    "stsb_ids = base_model.generate(input_ids,\n",
    "                               max_length=3,\n",
    "                               early_stopping=True)\n",
    "\n",
    "output = base_tokenizer.decode(stsb_ids[0], skip_special_tokens=True)\n",
    "print(f\"Similitud semántica? (1-5):\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa4ad67",
   "metadata": {},
   "source": [
    "## Q/A\n",
    "\n",
    "Esta tarea trata de dar respuesta a la pregunta realizada dando un contexto específico."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1168d18d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta:\n",
      "IPN\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    \"question: Where did Othon study engineering? context: Othon studied engineering at IPN, but he also took some \\\n",
    "        german courses at UNAM, and also took swimming lessons in Queretaro.\",\n",
    "                                    return_tensors=\"pt\").to(device)\n",
    "\n",
    "input_ids = base_model.generate(input_ids, early_stopping=True)\n",
    "output = base_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(f\"Respuesta:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83c89ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respuesta:\n",
      "UNAM\n"
     ]
    }
   ],
   "source": [
    "input_ids = base_tokenizer.encode(\n",
    "    \"question: Where did Othon learned german? context: Othon studied engineering at IPN, but he also took some \\\n",
    "        german courses at UNAM, and also took swimming lessons in Queretaro.\",\n",
    "                                    return_tensors=\"pt\").to(device)\n",
    "\n",
    "input_ids = base_model.generate(input_ids, early_stopping=True)\n",
    "output = base_tokenizer.decode(input_ids[0], skip_special_tokens=True)\n",
    "print(f\"Respuesta:\\n{output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52cc24e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fund-llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
